{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itolab/virtualenvs/yoshino-env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 25)           0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 100, 25)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100, 100)     50400       repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100, 50)      30200       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100, 44)      2244        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 82,844\n",
      "Trainable params: 82,844\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "841/841 [==============================] - 13s 15ms/step - loss: 0.7425 - acc: 0.0023\n",
      "Epoch 2/5\n",
      "841/841 [==============================] - 12s 14ms/step - loss: 0.6525 - acc: 0.0032\n",
      "Epoch 3/5\n",
      "841/841 [==============================] - 12s 14ms/step - loss: 0.6495 - acc: 0.0028\n",
      "Epoch 4/5\n",
      "841/841 [==============================] - 12s 14ms/step - loss: 0.6340 - acc: 0.0028\n",
      "Epoch 5/5\n",
      "841/841 [==============================] - 12s 14ms/step - loss: 0.7080 - acc: 0.0028\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 100, 44)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 100)          58000       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            505         lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            606         lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            606         lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 5)            505         lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            303         lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 60,525\n",
      "Trainable params: 60,525\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "841/841 [==============================] - 7s 9ms/step - loss: 0.5970 - dense_2_loss: 0.0989 - dense_3_loss: 0.0158 - dense_4_loss: 0.1414 - dense_5_loss: 0.1057 - dense_6_loss: 0.2352 - dense_2_acc: 0.6980 - dense_3_acc: 0.9941 - dense_4_acc: 0.2640 - dense_5_acc: 0.7182 - dense_6_acc: 0.3032\n",
      "Epoch 2/5\n",
      "841/841 [==============================] - 6s 8ms/step - loss: 0.5425 - dense_2_loss: 0.0866 - dense_3_loss: 2.0671e-04 - dense_4_loss: 0.1377 - dense_5_loss: 0.0936 - dense_6_loss: 0.2243 - dense_2_acc: 0.7015 - dense_3_acc: 1.0000 - dense_4_acc: 0.2806 - dense_5_acc: 0.7194 - dense_6_acc: 0.3389\n",
      "Epoch 3/5\n",
      "841/841 [==============================] - 6s 8ms/step - loss: 0.5419 - dense_2_loss: 0.0865 - dense_3_loss: 4.4261e-04 - dense_4_loss: 0.1376 - dense_5_loss: 0.0939 - dense_6_loss: 0.2234 - dense_2_acc: 0.7015 - dense_3_acc: 1.0000 - dense_4_acc: 0.2806 - dense_5_acc: 0.7194 - dense_6_acc: 0.3650\n",
      "Epoch 4/5\n",
      "841/841 [==============================] - 7s 8ms/step - loss: 0.5410 - dense_2_loss: 0.0866 - dense_3_loss: 2.8646e-04 - dense_4_loss: 0.1375 - dense_5_loss: 0.0934 - dense_6_loss: 0.2232 - dense_2_acc: 0.7015 - dense_3_acc: 1.0000 - dense_4_acc: 0.2806 - dense_5_acc: 0.7194 - dense_6_acc: 0.3567\n",
      "Epoch 5/5\n",
      "841/841 [==============================] - 7s 8ms/step - loss: 0.5411 - dense_2_loss: 0.0866 - dense_3_loss: 2.3660e-04 - dense_4_loss: 0.1370 - dense_5_loss: 0.0934 - dense_6_loss: 0.2238 - dense_2_acc: 0.7015 - dense_3_acc: 1.0000 - dense_4_acc: 0.2806 - dense_5_acc: 0.7194 - dense_6_acc: 0.3436\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_6 to have 3 dimensions, but got array with shape (100, 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b0ee7b6156a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;31m#pred = modelT2P.predict(train_txt[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;31m#actionPred = (pred[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelT2P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_txt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0mindexS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat2Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0muttr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/yoshino-env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/yoshino-env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_6 to have 3 dimensions, but got array with shape (100, 44)"
     ]
    }
   ],
   "source": [
    "#Converter from NL to Protocol\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import MeCab\n",
    "import codecs\n",
    "import json\n",
    "#\n",
    "actionS = ['estimate','comingout','divined','vote','None']\n",
    "agentS = ['Agent[01]','Agent[02]','Agent[03]','Agent[04]','Agent[05]','None']\n",
    "roleS = ['villager','seer','wolf','possesed','None']\n",
    "specieS = ['human','wolf','None']\n",
    "ACTION_NUM = len(actionS)#estimate,divine,co,vote\n",
    "AGENT_NUM =len(agentS)#15\n",
    "ROLE_NUM = len(roleS)#\n",
    "SPECIES_NUM = len(specieS)\n",
    "maxRemarkNum = 20\n",
    "maxWordNum = 100\n",
    "vocabNum = 1000#one-hotベクトルサイズ  \n",
    "#(発言数,プロトコル)->(発言数,単語数,語彙数)\n",
    "#(プロトコル)->(単語数,語彙数)\n",
    "class Parser:\n",
    "    \"\"\" MeCab を使用して文章を分割する \"\"\"\n",
    "    #listFlag = True#２次元リストで返すか(True)、１次元リストで返すか(False)\n",
    "    modeS = [1]#\n",
    "    tagger = None\n",
    "\n",
    "    def __init__(self,aModeS=None):# 第一引数は自インスタンス(self)\n",
    "        self.modeS = aModeS\n",
    "        self.tagger = MeCab.Tagger('')\n",
    "        #self.tagger = MeCab.Tagger('-d ../MeCab-Dict/')\n",
    "        parsed0 = self.tagger.parse('')#これ重要！！！！\n",
    "\n",
    "    def getParse(self,aText):#表層文字だけを分かち書きにして返す\n",
    "        #parsed0 = self.tagger.parse('')#これ重要！！！！\n",
    "        #node = self.tagger.parseToNode(aText)\n",
    "        #m = self.tagger(\"-Owakati\")\n",
    "        m = MeCab.Tagger(\"-Owakati\")\n",
    "        mecab_string = m.parse(aText)\n",
    "        mecab_list = []\n",
    "        for t in m.parse(aText).split():\n",
    "            mecab_list.append(t)\n",
    "        return mecab_list, mecab_string\n",
    "\n",
    "\n",
    "def prot2OneHot(aProtocol):\n",
    "    actionVec = np.zeros(ACTION_NUM)\n",
    "    action = aProtocol[0]\n",
    "    actionVec[actionS.index(action)]=1\n",
    "    #\n",
    "    subjectVec = np.zeros(AGENT_NUM)\n",
    "    subject = aProtocol[1]\n",
    "    subjectVec[agentS.index(subject)]=1\n",
    "    #\n",
    "    targetVec = np.zeros(AGENT_NUM)\n",
    "    target = aProtocol[2]\n",
    "    targetVec[agentS.index(target)]=1\n",
    "    #\n",
    "    roleVec = np.zeros(ROLE_NUM)\n",
    "    role = aProtocol[3]\n",
    "    roleVec[roleS.index(role)]=1\n",
    "    #\n",
    "    speciesVec = np.zeros(SPECIES_NUM)\n",
    "    species = aProtocol[4]\n",
    "    speciesVec[specieS.index(species)]=1\n",
    "    #\n",
    "    #protVec = np.concatenate([actionVec,subjectVec,targetVec,roleVec,speciesVec])\n",
    "    protVec = [actionVec,subjectVec,targetVec,roleVec,speciesVec]\n",
    "    protVec = [np.array(actionVec),np.array(subjectVec),np.array(targetVec),np.array(roleVec),np.array(speciesVec)]\n",
    "    return(protVec)\n",
    "\n",
    "def protS2OneHot(aProtocolList):\n",
    "    actionVecS = []\n",
    "    subjectVecS = []\n",
    "    targetVecS = []\n",
    "    roleVecS = []\n",
    "    speciesVecS = []\n",
    "    for protocol in aProtocolList:\n",
    "        protVec = prot2OneHot(protocol)\n",
    "        actionVecS.append(protVec[0])\n",
    "        subjectVecS.append(protVec[1])\n",
    "        targetVecS.append(protVec[2])\n",
    "        roleVecS.append(protVec[3])\n",
    "        speciesVecS.append(protVec[4])\n",
    "        #protVecS.append(protVec)\n",
    "    #actionArray = np.concatenate(actionVecS)\n",
    "    #subjectArray = np.concatenate(subjectVecS)\n",
    "    #targetArray = np.concatenate(targetVecS)\n",
    "    #roleArray = np.concatenate(roleVecS)\n",
    "    #speciesArray = np.concatenate(speciesVecS)\n",
    "    actionArray = np.array(actionVecS)\n",
    "    subjectArray = np.array(subjectVecS)\n",
    "    targetArray = np.array(targetVecS)\n",
    "    roleArray = np.array(roleVecS)\n",
    "    speciesArray = np.array(speciesVecS)\n",
    "    return(actionArray,subjectArray,targetArray,roleArray,speciesArray)\n",
    "\n",
    "mecab_parser = Parser()\n",
    "#chat_train_txt = 'chat_train.txt'\n",
    "#chat_train_prot = 'chat_prot.txt'\n",
    "divine_train_txt = 'divine_train2.txt'\n",
    "divine_train_prot = 'divine_prot2.txt'\n",
    "co_train_txt = 'co_train.txt'\n",
    "co_train_prot = 'co_prot.txt'\n",
    "vote_train_txt = 'vote_train.txt'\n",
    "vote_train_prot = 'vote_prot.txt'\n",
    "def txt2OneHot(aWordList,aVocab):\n",
    "    vocabNum = len(aVocab)\n",
    "    txtLen = len(aWordList)\n",
    "    oneHotS = []\n",
    "    vocab = list(aVocab)\n",
    "    for word in aWordList:\n",
    "        oneHotVec = np.zeros(vocabNum)\n",
    "        index = vocab.index(word)\n",
    "        oneHotVec[index] = 1\n",
    "        oneHotS.append(oneHotVec)\n",
    "    #oneHotArray = np.array(oneHotS)\n",
    "    return(oneHotS)\n",
    "\n",
    "def sent2OneHot(aSentList,aMaxWordNum,aVocab):\n",
    "    sentOneHot = []\n",
    "    for txt in aSentList:\n",
    "        oneHot = txt2OneHot(txt,aVocab)\n",
    "        while (len(oneHot) < maxWordNum):\n",
    "            vec = np.zeros(len(aVocab))\n",
    "            #print(np.shape(oneHot))\n",
    "            #print(np.shape(vec))\n",
    "            #oneHot = np.concatenate([oneHot,vec])\n",
    "            oneHot.append(vec)\n",
    "        sentOneHot.append(oneHot)\n",
    "    oneHotArray = np.array(sentOneHot)\n",
    "    return(oneHotArray) \n",
    "\n",
    "def makeDataTxt(aFileName):\n",
    "    textS = []\n",
    "    vocab_list = []\n",
    "    maxWordNum = 0\n",
    "    for line in codecs.open('./train3/'+aFileName, 'r', 'utf-8'):\n",
    "        parsed_list,parsed = mecab_parser.getParse(line)\n",
    "        #print(parsed)\n",
    "        vocab_list.extend(parsed_list)\n",
    "        textS.append(parsed_list)\n",
    "        maxWordNum = max(maxWordNum,len(parsed_list))\n",
    "    vocab = set(vocab_list)\n",
    "    return(textS,maxWordNum,vocab)\n",
    "\n",
    "def makeDataProt(aFileName):\n",
    "    protS = []\n",
    "    for line in codecs.open('./train3/'+aFileName, 'r', 'utf-8'):\n",
    "        protocol = line.split()\n",
    "        protS.append(protocol)\n",
    "    return(protS)\n",
    "\n",
    "#chat_textS,maxWordNum,vocab= makeDataTxt(chat_train_txt)\n",
    "#\n",
    "divine_textS,maxWordNum2,vocab= makeDataTxt(divine_train_txt)\n",
    "#vocabList = list(vocab)\n",
    "#vocabList2 = list(vocab2)\n",
    "#vocabList.extend(vocabList2)\n",
    "#vocab = set(vocabList)\n",
    "#maxWordNum = max(maxWordNum,maxWordNum2)\n",
    "#\n",
    "co_textS,maxWordNum2,vocab2= makeDataTxt(co_train_txt)\n",
    "vocabList = list(vocab)\n",
    "vocabList2 = list(vocab2)\n",
    "vocabList.extend(vocabList2)\n",
    "vocab = set(vocabList)\n",
    "maxWordNum = max(maxWordNum,maxWordNum2)\n",
    "#\n",
    "vote_textS,maxWordNum2,vocab2= makeDataTxt(vote_train_txt)\n",
    "vocabList = list(vocab)\n",
    "vocabList2 = list(vocab2)\n",
    "vocabList.extend(vocabList2)\n",
    "vocab = set(vocabList)\n",
    "maxWordNum = max(maxWordNum,maxWordNum2)\n",
    "#\n",
    "#chatTrain_txt = sent2OneHot(chat_textS,maxWordNum,vocab)\n",
    "divineTrain_txt = sent2OneHot(divine_textS,maxWordNum,vocab)\n",
    "coTrain_txt = sent2OneHot(co_textS,maxWordNum,vocab)\n",
    "voteTrain_txt = sent2OneHot(vote_textS,maxWordNum,vocab)\n",
    "#train_txt = np.concatenate([chatTrain_txt,divineTrain_txt,coTrain_txt,voteTrain_txt])\n",
    "train_txt = np.concatenate([divineTrain_txt,coTrain_txt,voteTrain_txt])\n",
    "#\n",
    "#train_protS = makeDataProt(chat_train_prot)\n",
    "train_protS = makeDataProt(divine_train_prot)\n",
    "#train_protS.extend(divine_protS)\n",
    "co_protS = makeDataProt(co_train_prot)\n",
    "train_protS.extend(co_protS)\n",
    "vote_protS = makeDataProt(vote_train_prot)\n",
    "train_protS.extend(vote_protS)\n",
    "actionProt,subjectProt,targetProt,roleProt,speciesProt = protS2OneHot(train_protS)\n",
    "#\n",
    "vocab_fileName = 'vocab.txt'\n",
    "def writeVocab():\n",
    "    #f = open(vocab_fileName, 'w')\n",
    "    f = codecs.open(vocab_fileName, 'w', 'utf-8')\n",
    "    for word in vocab:\n",
    "        f.write(word)\n",
    "    f.close()\n",
    "    return\n",
    "writeVocab()\n",
    "vocabNum = len(vocab)\n",
    "#==========================================\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input, Reshape, Embedding, Flatten, Dropout, RepeatVector\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import multi_gpu_model\n",
    "losseS = ['mean_squared_error', 'categorical_crossentropy']\n",
    "loss = losseS[0]\n",
    "def makeP2TModel():\n",
    "    actionInputs = Input(shape=(ACTION_NUM,))\n",
    "    subjectInputs = Input(shape=(AGENT_NUM,))\n",
    "    targetInputs = Input(shape=(AGENT_NUM,))\n",
    "    roleInputs = Input(shape=(ROLE_NUM,))\n",
    "    speciesInputs = Input(shape=(SPECIES_NUM,))\n",
    "    #verbInputs = Input(shape=(verb,))\n",
    "    #stateInputs = Input()\n",
    "    protocolInputs = Concatenate()([actionInputs,subjectInputs,targetInputs,roleInputs,speciesInputs])\n",
    "    #dayInputs = Input(1,)\n",
    "    #propertyInputs = Concatenate()([dayInputs])\n",
    "    repeatInputs = RepeatVector(maxWordNum)(protocolInputs)\n",
    "    lstm1 = LSTM(100,return_sequences=True)(repeatInputs)\n",
    "    lstm2 = LSTM(50,return_sequences=True)(lstm1)\n",
    "    remarkOut = Dense(vocabNum)(lstm2)\n",
    "    model = Model(inputs=[actionInputs, subjectInputs,targetInputs,roleInputs,speciesInputs], outputs=[remarkOut])\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return(model)\n",
    "modelP2T = makeP2TModel()\n",
    "batch_size = 5\n",
    "epochs=5\n",
    "#print(len(chatTrain_prot))\n",
    "modelP2T.fit([actionProt,subjectProt,targetProt,roleProt,speciesProt],train_txt,batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "json_string = modelP2T.to_json()\n",
    "fP2T = open('weightP2T.json', 'w')\n",
    "json.dump(json_string, fP2T)\n",
    "modelP2T.save_weights('paramP2T.hdf5')\n",
    "#print(modelP2T.predict([actionProt,subjectProt,targetProt,roleProt,speciesProt]))\n",
    "#model.load_weights('param.hdf5')\n",
    "def makeT2PModel():\n",
    "    remarkInput = Input(shape=(maxWordNum,vocabNum))\n",
    "    lstm1 = LSTM(100)(remarkInput)\n",
    "    actionOut = Dense(ACTION_NUM)(lstm1)\n",
    "    subjectOut = Dense(AGENT_NUM)(lstm1)\n",
    "    targetOut = Dense(AGENT_NUM)(lstm1)\n",
    "    roleOut = Dense(ROLE_NUM)(lstm1)\n",
    "    speciesOut = Dense(SPECIES_NUM)(lstm1)\n",
    "    model = Model(inputs=[remarkInput], outputs=[actionOut, subjectOut,targetOut,roleOut,speciesOut])\n",
    "    model.compile('adam', 'mean_squared_error', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return(model)\n",
    "modelT2P = makeT2PModel()\n",
    "modelT2P.fit(train_txt,[actionProt,subjectProt,targetProt,roleProt,speciesProt],batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "json_string = modelT2P.to_json()\n",
    "fT2P = open('weightT2P.json', 'w')\n",
    "json.dump(json_string, fT2P)\n",
    "modelT2P.save_weights('paramT2P.hdf5')\n",
    "#選択候補となる配列から値を取り出す\n",
    "def sample(aPreds,aTemperature=1.0):\n",
    "    #print(aPreds)\n",
    "    for i,val in enumerate(aPreds):\n",
    "        if(val<0):\n",
    "            aPreds[i] = 0\n",
    "        elif(val>1.0):\n",
    "            aPreds[i] = 1.0\n",
    "            #return(i)\n",
    "    aPreds = np.asarray(aPreds).astype('float64')#aPreds は0.0~1.0の値を取る実数配列\n",
    "    #aPreds = np.log(aPreds)/aTemperature\n",
    "    exp_preds = np.exp(aPreds)\n",
    "    #aPreds = exp_preds/np.sum(exp_preds)#aPredsの値の合計が１になるように正規化を行う\n",
    "    aPreds = aPreds/np.sum(aPreds)\n",
    "    probas = np.random.multinomial(1,aPreds,1)#multinomial 関数によって aPreds 内の値に応じた確率で試行を行って one-hot ベクトルにするs\n",
    "    return(np.argmax(probas))#numpy.argmax() で最大値要素の内で最も小さいインデックスを返す つまり、１である要素の添字を返す\n",
    "#\n",
    "def mat2Index(aMatrix):\n",
    "    indexS = []\n",
    "    for vec in aMatrix:\n",
    "        indexS.append(sample(vec))\n",
    "    return(indexS)\n",
    "\n",
    "#pred = modelT2P.predict(train_txt[-1])\n",
    "#actionPred = (pred[0])\n",
    "pred = modelT2P.predict(train_txt[-1])\n",
    "indexS = mat2Index(pred)\n",
    "uttr = ''\n",
    "for index in indexS:\n",
    "    uttr+=vocab[index]\n",
    "print(uttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 1., 0., 0.]]), array([[0., 0., 0., 0., 0., 1.]]), array([[0., 1., 0., 0., 0., 0.]]), array([[0., 0., 0., 0., 1.]]), array([[0., 1., 0.]])]\n",
      "(1, 100, 44)\n",
      "に占っなたら05ましよです投票のよです霊媒師の。人間投票占っ01COAgent占っ結果ようた占いよ霊媒師COや霊媒師村人だ結果05こそにん人狼結果[私こそ05て裏切り者霊媒師んますよ占っ結果だっはだはをた霊媒師です02にによ人狼02しは[の]こそだっです裏切り者は]先。はんだますだよは村人裏切り者こそにも03人狼03人間05ところだ先]\n"
     ]
    }
   ],
   "source": [
    "def sample(aPreds,aTemperature=1.0):\n",
    "    #print(aPreds)\n",
    "    #aPreds = aPreds[0]\n",
    "    for i,val in enumerate(aPreds):\n",
    "        if(val<0):\n",
    "            aPreds[i] = 0\n",
    "        elif(val>1.0):\n",
    "            aPreds[i] = 1.0\n",
    "            #return(i)\n",
    "    aPreds = np.asarray(aPreds).astype('float64')#aPreds は0.0~1.0の値を取る実数配列\n",
    "    #aPreds = np.log(aPreds)/aTemperature\n",
    "    exp_preds = np.exp(aPreds)\n",
    "    aPreds = exp_preds/np.sum(exp_preds)#aPredsの値の合計が１になるように正規化を行う\n",
    "    aPreds = aPreds/np.sum(aPreds)\n",
    "    probas = np.random.multinomial(1,aPreds,1)#multinomial 関数によって aPreds 内の値に応じた確率で試行を行って one-hot ベクトルにするs\n",
    "    return(np.argmax(probas))#numpy.argmax() で最大値要素の内で最も小さいインデックスを返す つまり、１である要素の添字を返す\n",
    "#\n",
    "def mat2Index(aMatrix):\n",
    "    indexS = []\n",
    "    for vec in aMatrix:\n",
    "        indexS.append(sample(vec))\n",
    "    return(indexS)\n",
    "pred = modelP2T.predict([actionProt[200:201],subjectProt[200:201],targetProt[200:201],roleProt[200:201],speciesProt[200:201]])\n",
    "print([actionProt[200:201],subjectProt[200:201],targetProt[200:201],roleProt[200:201],speciesProt[200:201]])\n",
    "print(np.shape(pred))\n",
    "indexS = mat2Index(pred[0])\n",
    "uttr = ''\n",
    "vocabList = list(vocab)\n",
    "for index in indexS:\n",
    "    uttr+=vocabList[index]\n",
    "print(uttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yoshino-Env",
   "language": "python",
   "name": "yoshino-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
